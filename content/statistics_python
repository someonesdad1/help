*StatisticsPython*

|Averages|
|Spread|
|Two_input_relations|
|NormalDist|
|NormalDistExamples|

Let x_i be a set of data points for i in [1, 2, ..., n]

------------------------------------------------------------------------------
Averages                                *Averages*

mean(data)              Arithmetic mean
fmean(data)             Fast floating point mean
geometric_mean(data)    Product of x_i, take 1/n root
harmonic_mean(data, weights=None)
    n/(1/x_1 + ... + 1/x_n)
median(data)            Uses mean of middle two (may be interpolated)
meadian_low(data)       Lower median, always in data set
meadian_high(data)      Higher median, always in data set
meadian_grouped(data, interval = 1)     See python docs
mode(data)              Most common data point
multimodemode(data)     Returns list of most frequently occurring values
    multimodemode("aabbbbccddddeeffffgg") returns ['b', 'd', 'f']
quantiles(data, *, n=4, method='exclusive')
    Divide data into n continuous intervals with equal probability.
    Returns a list of n-1 cut points separating the intervals.
    The number of data points should be > n.

    Example:
        data = [105, 129, 87, 86, 111, 111, 89, 81, 108, 92, 110,
                    100, 75, 105, 103, 109, 76, 119, 99, 91, 103, 129,
                    106, 101, 84, 111, 74, 87, 86, 103, 103, 106, 86,
                    111, 75, 87, 102, 121, 111, 88, 89, 101, 106, 95,
                    103, 107, 101, 81, 109, 104]
        print([round(q, 1) for q in quantiles(data, n=10)])
        results in 
            [81.0, 86.2, 89.0, 99.4, 102.5, 103.6, 106.0, 109.8, 111.0]

------------------------------------------------------------------------------
Spread                                *Spread*

pstdev(data, mu=None)   
    Population standard deviation.  If mu is given, use it to compute the
    square root of the second moment around a point that is not the mean.
pvariance(data, mu=None)
    Population variance.  If mu is given, use it to compute the second
    moment around a point that is not the mean.
stdev(data, xbar=None)
    Sample standard deviation, square root of the sample variance.
variance(data)
    Sample variance.  If you give xbar, it should be the mean of data.

------------------------------------------------------------------------------
Two_input_relations                                *Two_input_relations*

covariance(x, y)        Covariance of x and y
correlation(x, y)       Pearson's correlation coefficient on [-1, 1]
linear_regression(x, y) Returns [slope, intercept]

------------------------------------------------------------------------------
NormalDist                                *NormalDist*

This is a tool for creating and manipulating normal distributions of a
random variable. It is a class that treats the mean and standard deviation
of data measurements as a single entity.

NormalDist(mu=0.0, sigma=1.0)

Properties
    mean
    median
    mode
    variance

Methods
    from_samples(data)   [Class Method]
        Constructs a NormalDist instance with mu and sigma parameters
        estimated from the data using fmean() and stdev().

    samples(n, *, seed=None)
        Return a list of floats that are a random sample.

    pdf(x)
        Compute the relative likelihood that a random variable X will be
        near x.

    cdf(x)
        Compute P(X <= x).

    inv_cdf(p)
        Compute the inverse CDF of p such that cdf(result) is p.

    overlap(other)
        Return a value between 0 and 1 that measured the overlap of two
        normal distributions.

    quantiles(n=4)
        Return a list of n-1 cutpoints representing the n continuous
        intervals that have equal probability.

    zscore(x)
        Compute the standard score of x which is (x - mean)/stdev.

Instances of NormalDist support addition, subtraction, multiplication and
division by a constant. These operations are used for translation and
scaling.

You can add two instances.  Example:
    >>> birth_weights = NormalDist.from_samples([2.5, 3.1, 2.1, 2.4, 2.7, 3.5])
    >>> drug_effects = NormalDist(0.4, 0.15)
    >>> combined = birth_weights + drug_effects
    >>> round(combined.mean, 1)
    3.1
    >>> round(combined.stdev, 1)
    0.5

------------------------------------------------------------------------------
NormalDistExamples                                *NormalDistExamples*

NormalDist readily solves classic probability problems.

For example, given historical data for SAT exams showing that scores are
normally distributed with a mean of 1060 and a standard deviation of 195,
determine the percentage of students with test scores between 1100 and
1200, after rounding to the nearest whole number:

    >>> sat = NormalDist(1060, 195)
    >>> fraction = sat.cdf(1200 + 0.5) - sat.cdf(1100 - 0.5)
    >>> round(fraction * 100.0, 1)
    18.4

Find the quartiles and deciles for the SAT scores:

    >>> list(map(round, sat.quantiles()))
    [928, 1060, 1192]
    >>> list(map(round, sat.quantiles(n=10)))
    [810, 896, 958, 1011, 1060, 1109, 1162, 1224, 1310]

To estimate the distribution for a model than isn’t easy to solve
analytically, NormalDist can generate input samples for a Monte Carlo
simulation:

    >>> def model(x, y, z):
    ...     return (3*x + 7*x*y - 5*y) / (11 * z)
    ...
    >>> n = 100_000
    >>> X = NormalDist(10, 2.5).samples(n, seed=3652260728)
    >>> Y = NormalDist(15, 1.75).samples(n, seed=4582495471)
    >>> Z = NormalDist(50, 1.25).samples(n, seed=6582483453)
    >>> quantiles(map(model, X, Y, Z))       
    [1.4591308524824727, 1.8035946855390597, 2.175091447274739]

Normal distributions can be used to approximate Binomial distributions when
the sample size is large and when the probability of a successful trial is
near 50%.

For example, an open source conference has 750 attendees and two rooms with
a 500 person capacity. There is a talk about Python and another about Ruby.
In previous conferences, 65% of the attendees preferred to listen to Python
talks. Assuming the population preferences haven’t changed, what is the
probability that the Python room will stay within its capacity limits?

    >>> n = 750             # Sample size
    >>> p = 0.65            # Preference for Python
    >>> q = 1.0 - p         # Preference for Ruby
    >>> k = 500             # Room capacity

    >>> # Approximation using the cumulative normal distribution
    >>> from math import sqrt
    >>> round(NormalDist(mu=n*p, sigma=sqrt(n*p*q)).cdf(k + 0.5), 4)
    0.8402

    >>> # Solution using the cumulative binomial distribution
    >>> from math import comb, fsum
    >>> round(fsum(comb(n, r) * p**r * q**(n-r) for r in range(k+1)), 4)
    0.8402

    >>> # Approximation using a simulation
    >>> from random import seed, choices
    >>> seed(8675309)
    >>> def trial():
    ...     return choices(('Python', 'Ruby'), (p, q), k=n).count('Python')
    >>> mean(trial() <= k for i in range(10_000))
    0.8398

Normal distributions commonly arise in machine learning problems.

Wikipedia has a nice example of a Naive Bayesian Classifier. The challenge
is to predict a person’s gender from measurements of normally distributed
features including height, weight, and foot size.

We’re given a training dataset with measurements for eight people. The
measurements are assumed to be normally distributed, so we summarize the
data with NormalDist:

    >>> height_male = NormalDist.from_samples([6, 5.92, 5.58, 5.92])
    >>> height_female = NormalDist.from_samples([5, 5.5, 5.42, 5.75])
    >>> weight_male = NormalDist.from_samples([180, 190, 170, 165])
    >>> weight_female = NormalDist.from_samples([100, 150, 130, 150])
    >>> foot_size_male = NormalDist.from_samples([12, 11, 12, 10])
    >>> foot_size_female = NormalDist.from_samples([6, 8, 7, 9])

Next, we encounter a new person whose feature measurements are known but
whose gender is unknown:

    >>> ht = 6.0        # height
    >>> wt = 130        # weight
    >>> fs = 8          # foot size

Starting with a 50% prior probability of being male or female, we compute
the posterior as the prior times the product of likelihoods for the feature
measurements given the gender:

    >>> prior_male = 0.5
    >>> prior_female = 0.5
    >>> posterior_male = (prior_male * height_male.pdf(ht) *
    ...                   weight_male.pdf(wt) * foot_size_male.pdf(fs))

    >>> posterior_female = (prior_female * height_female.pdf(ht) *
    ...                     weight_female.pdf(wt) * foot_size_female.pdf(fs))

The final prediction goes to the largest posterior. This is known as the
maximum a posteriori or MAP:

    >>> 'male' if posterior_male > posterior_female else 'female'
    'female'
